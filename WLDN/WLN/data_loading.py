import itertools
from random import shuffle

import numpy as np
import pandas as pd
from tensorflow.keras.utils import Sequence

from graph_utils.ioutils_direct import get_changing_label, smiles2graph_list_bin, reactant_tracking
from graph_utils.mol_graph_useScores import smiles2graph as s2g_edit


def convert_data(filename, reactants_only=True):
    '''
    Function to convert smiles and edits text file to a pandas dataframe

    Args:
        filename (str): path to the text file
        reactants_only (bool): True returns only reactant smiles, False returns reaction smiles

    Output:
        dataframe: A pandas dataframe with columns "smiles" and "bond_edits"
    '''

    df = pd.read_csv(filename, sep=' ', names=['smiles', 'bond_edits'])
    df_edits = df['bond_edits']
    df_smiles = df['smiles']
    if reactants_only:
        return df_smiles.str.split('>>', expand=True)[0], df_edits
    else:
        #TODO what do we do if no rxn symbol?
        return df['smiles'], df_edits


def convert_detail_data(filename, filename_detail):
    '''
    Function to convert ground truth edits and edits generated by the NN to lists for training

    Args:
        filename (str): path to the text file of smiles and edits
        filename_detail (str): path to smiles, edits, and scores generated by gen_cand_score

    Output
        list: a list of lists of smiles, candidates, and edits
    '''
    smiles, edits = convert_data(filename, reactants_only=False)
    smiles = smiles.str.split('>>', expand=True)
    r, p = smiles[0], smiles[1]
    with open(filename_detail, 'r') as f:
        cand = f.read().splitlines()
    data_dict = {x.strip("\r\n ").split()[0]:x.strip("\r\n ").split()[1:] for x in cand if len(x)>0}

    fail = 0
    train = []
    for i,j in enumerate(r):
        try:
        # TODO add failsafes?
            train.append([j + '>>' + p[i], data_dict[j], edits[i]])
        except KeyError:
            fail += 1

    if fail > 10:
        print('!' * 100)
        print('!' * 100)
        print(f'WARNING: CONVERTING DETAILED DATA HAD {fail} FAILURES OUT OF {len(smiles)} EXAMPLES')
        print('!' * 100)
        print('!' * 100)

    return zip(*train)


class FileLoader(Sequence):
    """
    Class for loading data from files in chunks.

    For optimal performance, batches should be requested in order and NOT shuffled.
    This class will automatically shuffle chunk indices and data within each chunk.
    """

    def __init__(self, filename, batch_size, chunk_size, shuffle):

        assert chunk_size % batch_size == 0, 'chunk_size should be integer multiple of batch_size'

        self.filename = filename
        self.batch_size = batch_size
        self.chunk_size = chunk_size
        self.shuffle = shuffle

        # The current true chunk index
        self._current_chunk_index = 0
        self._current_chunk = None

        # Get total lines in file
        with open(self.filename, 'r') as f:
            self.dataset_size = sum(1 for _ in f)
            # For mapping an apparent chunk index to the "true" index for shuffling
            self.chunk_indices = list(range(int(np.ceil(self.dataset_size / self.chunk_size))))

        self.on_epoch_end()

    def __len__(self):
        """Total number of batches in dataset."""
        return int(np.ceil(self.dataset_size / self.batch_size))

    def __getitem__(self, index):
        """Return a specific batch by index."""
        chunk = self.get_chunk(index)
        local_index = index % (self.chunk_size // self.batch_size)
        start = local_index * self.batch_size
        end = (local_index + 1) * self.batch_size
        batch = chunk[start:end]
        return self.data_generation(batch)  # x, y

    def on_epoch_end(self):
        """Shuffle chunk indices after each epoch."""
        if self.shuffle:
            shuffle(self.chunk_indices)

    def get_chunk(self, batch_index):
        """Return a specific chunk based on the batch index.

        If the requested batch is not in the currently loaded chunk,
        then a new chunk is loaded from file.
        """
        chunk_index = batch_index * self.batch_size // self.chunk_size
        if self.dataset_size % self.chunk_size != 0:
            # chunks are being shuffled, and hence a truncated chunk can end up halfway and mess up count
            # -> you need to correct for the length of this chunk
            index_truncated_chunk = self.chunk_indices.index(len(self.chunk_indices) - 1)
            if chunk_index >= index_truncated_chunk:
                chunk_index = ((batch_index * self.batch_size - self.dataset_size % self.chunk_size) // self.chunk_size) + 1

        true_chunk_index = self.chunk_indices[chunk_index]

        if true_chunk_index == self._current_chunk_index and self._current_chunk is not None:
            return self._current_chunk
        else:
            # Load a new chunk
            start = true_chunk_index * self.chunk_size
            end = (true_chunk_index + 1) * self.chunk_size
            if end > self.dataset_size:
                end = self.dataset_size
            with open(self.filename, 'r') as f:
                chunk = list(itertools.islice(f, start, end))
            if self.shuffle:
                # This shuffles data within the chunk
                shuffle(chunk)
            self._current_chunk_index = true_chunk_index
            self._current_chunk = chunk
            return self._current_chunk


class Graph_DataLoader(FileLoader):
    '''
    A class for dataloading for wln direct (WLNPairwiseAtomClassifier)\

    Args:
        filename (str): path to file containing reaction smiles and correct bond edits
        batch_size (int): size of the batches for training/valid/test
        shuffle (bool): at the end of the epoch whether to shuffle the data or not
        reagents (bool): track reagents (ie molecules that do not contribute atoms to product) during predictions

    Output
        generator: a generator that will produce "batch_size" batches of reactant graphs and labels
    '''

    def __init__(self, filename, batch_size=10, chunk_size=1000, shuffle=True, detailed=False, reagents=False):
        super().__init__(filename, batch_size, chunk_size, shuffle)
        self.detailed = detailed
        self.reagents = reagents

    def data_generation(self, batch):
        smiles_tmp, labels_tmp = zip(*(x.strip().split() for x in batch))
        r_tmp = [x.split('>')[0] for x in smiles_tmp]
        graph_inputs = list(smiles2graph_list_bin(r_tmp, idxfunc=lambda x: x.GetIntProp('molAtomMapNumber') - 1))
        max_natoms = graph_inputs[0].shape[1]
        bond_labels = []
        sp_labels = []
        for smi, edit in zip(r_tmp, labels_tmp):
            l = get_changing_label(smi, edit, max_natoms)
            bond_labels.append(l[0])
            sp_labels.append(l[1])

        if self.detailed:
            all_ratoms, all_rbonds = reactant_tracking(smiles_tmp, hard=self.reagents)
            return graph_inputs, np.array(bond_labels), np.array(sp_labels), all_ratoms, all_rbonds, smiles_tmp, labels_tmp
        else:
            return graph_inputs, np.array(bond_labels)


class Candidate_DataLoader(FileLoader):
    '''
    Class that load the data for the Candidate Ranker

    Args:
        filename (str): path to file containing reaction smiles, correct bond edits, and candidate bond edits
        batch_size (int): size of the batches for training/valid/test (only 1 supported right now)
        cutoff (int): number of candidate products to consider
        core_size (int): number of bonds to retain in reactive core
        shuffle (bool): at the end of the epoch whether to shuffle the data or not
        testing (bool): whether the generator is part of a model evaluation or training

    Output
        generator: a generator that will produce "batch_size" batches of graphs
    '''
    def __init__(self, filename, batch_size=1, chunk_size=10000, cutoff=150, core_size=16, shuffle=True, testing=False):
        super().__init__(filename, batch_size, chunk_size, shuffle and not testing)
        self.cutoff = cutoff
        self.core_size = core_size
        self.testing = testing

    def data_generation(self, batch):
        smiles_tmp, labels_tmp, cand_type_tmp, cand_change_tmp, cand_prob_tmp = [], [], [], [], []
        for x in batch:
            parts = x.strip().split()
            smiles_tmp.append(parts[0])
            labels_tmp.append(parts[1])
            cand_type_tmp.append(parts[2::3])
            cand_change_tmp.append(parts[3::3])
            cand_prob_tmp.append(parts[4::3])

        cand_bond, cand_h, cand_f = [], [], []
        for cand_type, cand, prob in zip(cand_type_tmp, cand_change_tmp, cand_prob_tmp):
            b_indices = [index for index, letter in enumerate(cand_type[:self.core_size]) if letter == 'B']
            h_indices = [index for index, letter in enumerate(cand_type[:self.core_size]) if letter == 'H']
            f_indices = [index for index, letter in enumerate(cand_type[:self.core_size]) if letter == 'F']
            bonds = []
            for i in b_indices:
                x, y, t = cand[i].split('-')
                x, y = tuple(sorted([int(float(x)) - 1, int(float(y)) - 1]))
                bonds.append((x, y, float(t), float(prob[i])))
            cand_bond.append(bonds)

            Hs = []
            for i in h_indices:
                x, t = cand[i].split(':')
                x = int(float(x)) - 1
                Hs.append((x, float(t), float(prob[i])))
            cand_h.append(Hs)

            FCs = []
            for i in f_indices:
                x, t = cand[i].split(':')
                x = int(float(x)) - 1
                FCs.append((x, float(t), float(prob[i])))
            cand_f.append(FCs)

        gold_bond, gold_h, gold_f = [], [], []
        for edits in labels_tmp:
            bond_label, h_label, f_label = set(), set(), set()
            bond_edits, h_edits, f_edits = edits.split('/')

            if bond_edits:
                for bond in bond_edits.split(';'):
                    x, y, t = bond.split('-')
                    x, y = tuple(sorted([int(float(x)) - 1, int(float(y)) - 1]))
                    bond_label.add((x, y, float(t)))
            gold_bond.append(bond_label)

            if h_edits:
                for h in h_edits.split(';'):
                    x, t = h.split(':')
                    x = int(float(x)) - 1
                    h_label.add((x, float(t)))
            gold_h.append(h_label)

            if f_edits:
                for f in f_edits.split(';'):
                    x, t = f.split(':')
                    x = int(float(x)) - 1
                    f_label.add((x, float(t)))
            gold_f.append(f_label)


        # Make sure batch size is 1 because this will not train otherwise
        assert len(smiles_tmp) == 1

        r, _, p = smiles_tmp[0].split('>')


        if not self.testing:
            # try:
            graph_inputs, _ = s2g_edit(r, p, cand_bond[0],cand_h[0], cand_f[0], gold_bond[0],gold_h[0],gold_f[0], cutoff=self.cutoff)
            # except:
            #     # print(r, p, cand_bond[0],cand_h[0], cand_f[0], gold_bond[0],gold_h[0],gold_f[0])
            #
            #     print('\n reactant: ', r)
            #     print('\n product: ', p)
            #     print('\n bond: ',cand_bond[0])
            #     print('\n hydro: ',cand_h[0])
            #     print('\n formal: ', cand_f[0])
            #     pass
        else:
            graph_inputs, conf = s2g_edit(r, None,cand_bond[0],cand_h[0], cand_f[0],None, None, None, cutoff=self.cutoff, \
                                      idxfunc=lambda x: x.GetIntProp('molAtomMapNumber') - 1, testing=self.testing)

        inputs = (np.expand_dims(graph_inputs[0], axis=0),
            np.expand_dims(graph_inputs[1], axis=0),
            np.expand_dims(graph_inputs[2], axis=0),
            np.expand_dims(graph_inputs[3], axis=0),
            np.expand_dims(graph_inputs[4], axis=0),
            np.expand_dims(graph_inputs[5], axis=0))

        if not self.testing:
            return inputs, np.expand_dims(graph_inputs[6], axis=0)
        else:
            return inputs, conf, r
